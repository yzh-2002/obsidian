自我介绍：
面试官您好，我是杨子涵，目前研一，就读于电子科技大学信息与通信工程学院，从24年初开始接触推荐算法，独立完成了天池上新闻推荐学习赛，目前正在做Kaggle上OTTO 多目标预测推荐赛，经过大半年的学习和项目联系，目前对推荐系统的整体流程有了一定了解，希望能够通过实习积累自己这方面的工程实践能力。
## 机器学习
1. 二分类损失函数为什么是CE，而不是MSE？
	1. 数据角度：MSE损失函数考虑了各个分类的输出结果，使得预测的越平均越好（不适合分类问题），CE仅关注正确分类的输出结果，更适合于分类问题
	2. 优化角度：CE中损失函数关于最后一层输出的梯度为`y-1`，MSE则是`-2y(1-y)^2`，当`y=0`时，分类严重错误，但MSE梯度为0
2. AUC：ROC曲线与坐标轴围成的面积大小
	1. ROC曲线：横轴是FPR（反例的误判率），纵轴是TPR（正例的查全率）
	2. PR使用查全率和精准率评估分类器好坏，精准率很多场景没有意义，原因在于大多数场景中反例较少，模型预测全部判正，精准率也不低。实际情况下更看重反例的误判率。
4. GBDT？
	1. `Adaboost`：其boosting过程是改变训练数据权重，每次只训练一个个体学习器
	2. `GBDT`采用加法模型，每次训练都在所有已有学习器上训练新的弱学习器的参数
	3. 核心思想是让新的弱学习器去拟合`损失函数关于已有学习器的负梯度`（把损失函数看作`f(x)`的函数）
5. XGBoost？
6. LightGBM？
7. bagging与boosting？
	1. boosting：串行训练弱分类器
	2. bagging：各分类器之间可并行训练，最终结果进行合并

## 推荐算法
1. DIN模型
2. MMOE gate输出形状与什么有关？