题目：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410181953392.png)

---
[参考链接](https://blog.csdn.net/sinat_37853238/article/details/127078940)
## 内核编程

内核开发时，通过`uname -r`查看当前主机的内核版本（内核编程与内核版本紧密相连），然后在`usr/src`查看是否包含对应内核版本的代码。不存在需要下载，之后便可以开发了。

`VSCode`编写代码时可能会提示各种错误（找不到头文件等等），贴一份`c_cpp_properties.json`：
```json
{ 
	"configurations": [ 
		{ 
		"name": "Linux", 
		"includePath": [ 
			"${workspaceFolder}/**", 
			"/usr/src/linux-headers-6.5.0-28-generic/", 
			"/usr/src/linux-headers-6.5.0-28-generic/include/", 
			"/usr/src/linux-headers-6.5.0-28-generic/arch/x86/include/",   "/usr/src/linux-headers-6.5.0-28-generic/arch/x86/include/generated/",     "/usr/src/linux-hwe-6.5-headers-6.5.0-28/arch/**/include/", 
			"/usr/src/linux-hwe-6.5-headers-6.5.0-28/include/" 
		], 
		// 参考链接：https://stackoverflow.com/questions/71735229
		"defines": ["__KERNEL__","MODULE"], 
		"compilerPath": "/usr/bin/gcc", 
		"cStandard": "c17", 
		"cppStandard": "gnu++17", 
		"intelliSenseMode": "linux-gcc-x64" 
		} 
	], 
	"version": 4 
}
```

内核编程：`Hello World`
```c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>

//模块许可证声明,必须
MODULE_LICENSE("GPL");

//模块加载函数,必须
static int hello_init(void){
    printk(KERN_ALERT "Hello Kernel!");
    return 0;
}
// 另一种写法，较之上面写法省内存，原因其初始化之后释放该函数及其所占内存
// static int __init hello_init(void){}

//模块卸载函数,必须
static void hello_exit(void){
	printk(KERN_ALERT "Goodbye Kernel/n");
}
// static void __exit hello_exit(void){}

//模块的注册
module_init(hello_init);
module_exit(hello_exit);
```

内核编程实际上是对于Linux可加载**内核模块**的编写，内核模块是具有独立功能的程序，可单独编译，但不能单独运行，其运行必须被链接到内核作为内核的一部分在内核空间中运行。

如何将编写的内核模块加载到内核呢？
1. 静态编译进内核，内核启动时自动加载
2. 使用`insmod`命令将模块动态加载到正在运行的内核，不需要时用`rmmod`命令将模块卸载

再回到上面的`Hello World`代码，不同于应用编程中以`main`函数作为入口，模块采用`module_init`作为模块入口，`module_exit`指定了模块退出时的出口函数。

### Makefile
> 可以在终端中使用`gcc`编译代码生成可执行文件，但内核编写时，由于需要显式指定内核版本等信息，通过`Makefile`要比`gcc`方便很多

`gcc`是编译器，`make`是一个命令工具，用于解释`Makefile`中的指令（从而执行一组`gcc/g++`为主的shell命令序列）。

内核编程的`Makefile`示例：
```Makefile
# -m表示其为一个模块 .o目标文件（机器码，与所需库文件链接在一起即可构成可执行文件）
obj-m += hello.o
CURRENT_PATH:=$(shell pwd)
LINUX_KERNEL:=$(shell uname -r)
LINUX_KERNEL_PATH:=/usr/src/linux-headers-$(LINUX_KERNEL)

all:
	# -C 指定读取Makefile文件的目录
	# M 指定模块源代码的位置
	# modules 表示构建目标是内核模块
    make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules
clean:
    make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean
```

经编译之后生成如下文件：
1. `hello.o`：模块的目标文件，生成`hello.ko`文件的基础
2. `hello.ko`：**最终生成的内核模块代码，可被内核加载和执行**
	1. `insmod xxx.ko`：将模块加载入内核
	2. `rmmod xxx.ko`：将模块从内核中卸载
3. `hello.mod`：
4. `hello.mod.c`
5. `hello.mode.o`
6. `Modules.symvers`
7. `modules.order`

常见问题：
1. `Key was rejected by service`：
	1. `Linux内核`从3.7开始加入了模块签名检查机制，加载模块时内核会检查模块的签名，如果签名不存在或签名内容不一致，会强制退出模块的加载。
	2. 在`/usr/src/linux-$(uname -r)`下执行`make menuconfig`可进入内核配置项，目前已经无法关闭内核模块签名校验的配置项
	3. 签名方法：[参考链接](https://ubuntu.com/blog/how-to-sign-things-for-secure-boot)

## 设计思路
题目要求在IP模块和以太网接口之间串接一个虚拟的`vni0`接口，但是笔记本只有无线网口，没有以太网口，暂时以无线网口实现（应该区别不大，毕竟只是端侧，不涉及数据处理...）

**发送数据**：Linux内核IP模块送下的IP分组（到vni0），封装一个vni头部和以太网帧头部发送给eth0
1. 正常情况下，Linux内核IP模块发送数据包要走哪个网络设备接口是根据路由表来确定的，为了保证其能送到`vni0`接口，可通过`netfilter hook`将其转发过去
2. `vni0`接口接受该数据包之后，封装vni hdr和以太网帧头部再发送给`eth0`
	1. 设置`net_device->netdev_ops`中的`.ndo_start_xmit`和`.ndo_recv`？？？

**接受数据**：eth0收到的vni分组的vni头部去掉，然后将IP分组上交给Linux内核的IP模块
1. 主机在接受数据包时，为了保证由`eth0`接口收到，需要确保发送数据包的源mac地址与`eth0`接口`mac`地址一致
2. 

任务要求：利用`ping`命令发出100个ICMP报文，统计VNI模块发送和接收的分组个数，每分钟定时打印以下信息：
1. 发送端发送分组总数，每分钟的发送速率
2. 接收端的接受分组总数，每分钟的发送速率



### 网络协议栈
> [Linux 网络协议栈](https://github.com/0voice/linux_kernel_wiki/tree/main/%E6%96%87%E7%AB%A0/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%A0%88)

`sk_buff`是linux内核的网络代码中最重要的数据结构，用来表示已接收或将要传输的数据包，在不同的网络层被使用（`L2 MAC | L3 IP | L4 TCP/UDP`）。
1. 数据包在传输时，会追加头信息
2. 数据包在接收时，buffer会从下层传送到上层，此时前一层的协议头对于当前层来说已经没用了，例如在L3层内核可以通过`skb_network_header`获取指向当前协议头的内存起始地址

### 网络设备驱动

`net_device`：网络设备接口层为千变万化的网络设备提供统一，抽象的数据结构，`net_device`在内核中指代一个网络设备（无论真实设备还是虚拟设备）。
```c
struct net_device *alloc_netdev(
	int sizeof_priv, 
	const char *name,  //设备名称的格式化字符串1
	unsigned char name_assign_type, // 分配类型
	void (*setup)(struct net_device *) //设置函数，ether_setup是一个用于初始化以太网设备的标准函数
	);
```

网络设备对应于二层转发时的交换机接口，默认主机有`docker0,lo,wlp0s20f3`三个网络设备接口。
1. `lo`：本地回环接口，用于本地通信，不再赘述
2. `docker0`：Docker创建的**虚拟网络接口**，以便于容器可以通过其与主机通信
3. `wlp0s20fs`：无线网络接口（对应笔记本的物理网卡）
	1. 笔记本没有以太网络接口，只有无线网络接口

### Netlink
> Netlink Socket 是用于实现用户进程与内核进程通信的一种特殊的进程间通信方式，也是网络应用程序与内核通信的最常用的接口。

用户态应用使用标准的`Socket API`便可以使用`netlink`提供的强大功能，内核态需要使用专门的`API`来使用`netlink`

标准`socket`编程流程：
![](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/blog/202410261304751.png)

---
为什么服务端需要`bind`，而客户端不需要呢？
服务端需要等待客户端的连接请求，故需要一个固定的IP地址和端口来对外提供服务。
客户端目的是发起连接请求，通常不需要绑定固定的地址和端口，操作系统会自动为客户端socket分配一个临时的，本地未占用的端口来完成连接，该临时端口会在连接断开后被系统释放或复用。


用户态：
```c
// netlink地址（类比于socket编程中sockaddr_in）
struct sockaddr_nl {
	_kernel_sa_family_t nl_family; //AF_NETLINK
	unsigned shrot nl_pad; // 0
	_u32 nl_pid; // 发送者的pid，可通过`getpid`获取，内核作为发送者时设置为0
	_u32 nl_groups; //multicast group mask,0表示不加入任何多播组
}

// netlink消息头，随后紧跟的一片内存空间是payload
struct nlmsghdr {
	_u32 nlmsg_len; //消息长度，单位字节，包含消息头+payload
	_u16 nlmsg_typel // 消息的类型
	_u16 nlmsg_flags; // 附加在消息上的额外信息说明
	_u32 nlmsg_seq; // 
	_u32 nlmsg_pid; // 
}
```
`sockaddr_nl`中`nl_pid`作用是用于唯一的标识一个基于`netlink`的`socket`通道。通常情况下`nl_pid`都设置为当前进程的进程号，设置为0时一般指内核。

```c
// socket 创建函数
int socket (
	int __domain, //协议族 
	int __type, // socket类型，例如：SOCK_STREAM（TCP），SOCK_RAW（原始套接字）
	int __protocol
	);

// 将socket与特定的本地地址绑定
int bind (int __fd, __CONST_SOCKADDR_ARG __addr, socklen_t __len);

// 发送数据
ssize_t sendto (int __fd, const void *__buf, size_t __n,
		       int __flags, __CONST_SOCKADDR_ARG __addr,
		       socklen_t __addr_len);

// 接收数据
ssize_t recvfrom (int __fd, void *__restrict __buf, size_t __n,
			 int __flags, __SOCKADDR_ARG __addr,
			 socklen_t *__restrict __addr_len);

// 关闭socket
int close (int __fd);
```

用户态使用`netlink`与内核进行`socket`通信与标准`socket`通信的不同之处在于：
1. `socket`地址结构不同
2. 发送数据时，`netlink`多了一个消息头结构

---
内核态：
```c
// 创建内核netlink socket所需的配置参数结构体
struct netlink_kernel_cfg {
    unsigned int    groups;  
    unsigned int    flags;  
    void        (*input)(struct sk_buff *skb); // 收到信息后的回调函数 
    struct mutex    *cb_mutex; 
    void        (*bind)(int group); 
    bool        (*compare)(struct net *net, struct sock *sk);
};
```

```c
// 
struct sock * netlink_kernel_create(
	struct net *net, // net 网络命名空间，一般默认传入 &init_net 
	int unit, // netlink 协议类型
	struct netlink_kernel_cfg *cfg // cfg 存放配置参数，一般仅设置回调函数
	);

/* 发送单播消息 */
int netlink_unicast(
	struct sock *ssk, //上面函数返回的socket
	struct sk_buff *skb, //skb buff 指针，可以通过skb获得nlmsghdr及后续的payload
	__u32 portid, //
	int nonblock //表示该函数是否为非阻塞，如果为1，该函数将在没有接收缓存可利用时立即返回，而如果为0，该函数在没有接收缓存可利用定时睡眠
	);
```
关于`netlink_kernel_create`的`unit`参数说明：
linux有一些默认的`netlink`协议类型（可见`linux/netlink.h`定义），相同协议类型的`socket`才能在用户态与核心态之间通信，用于可以在未被使用的协议号中选择（最大值为32，一般选择内核为我们预留的`NETLINK_GENERIC`），也可以自定义一个（一般自定义的数值要大于22，因为22及之前的`netlink`已经定义过了...）

### NetFilter 
`netfilter`运行在内核中，是`linux`内置的一种防火墙机制，其如何对网络数据包进行干预呢？**在网络协议栈中预留钩子（`hooks`），这些hook允许内核模块注册回调函数**。

整个网络协议栈如下图所示：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410241500621.png)

---
1. `Driver`是网卡驱动程序，网络链路层**靠近硬件一侧**的接口
2. `Device`代表网络设备，不是物理上的设备，而是一个抽象，网络链路层**靠近软件一侧**的接口

在上面网络协议的基础上，`netfliter`是如何运作的可见下图：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410241500066.png)

---
1. `prerouting hook`：
2. `forward hook`：经路由后，发现数据包不是发往本机的，本机仅起到路由作用时触发
3. `input hook`：经路由后，发现数据包是发往本机时触发，**可用于加工发往本机的数据包**
4. `output hook`：用于加工本地进程输出的数据包
5. `postrouting hook`：

如何在`netfilter`提供的hook中注册函数呢？

首先，`netfilter`框架为多种协议提供了一套hooks，用`struct list_head nf_hooks[NPROTO][NF_MAX_HOOKS]`二维数组结构存储，一维为协议族，二维为hook点，有五个hook点（与上面的hook对应，名称略有区别）：
1. `NF_IP_PRE_ROUTING`：
2. `NF_IP_LOCAL_IN`：
3. `NF_IP_FORWARD`：
4. `NF_IP_POST_ROUTING`：
5. `NF_IP_LOCAL_OUT`：

hook函数的编写规范如下（**三个参数，一个返回值**）：
```c
unsigned int my_custom_hook(
	void *priv,
	struct sk_buff *skb,
	const struct nf_hook_state *state
){
	// TODO
	return NF_ACCEPT;
}
```

每个注册的`hook`函数经过处理后返回值均为下列之一，告知`netfilter`核心代码处理结果，以便对报文采取相应的动作：
1. `NF_ACCEPT`：继续正常的报文处理
2. `NF_DROP`：丢弃报文
3. `NF_STOLEN`：告诉`netfilter`忘掉该数据包，hook函数获取该数据包的所有权，由hook函数控制数据包的生命周期
4. `NF_QUEUE`：报文入队，通常交由用户程序处理
5. `NF_REPEAT`：再次调用该hook函数，谨慎使用，避免死循环

写完`hook`函数，就可以调用`nf_register_net_hook()`向`netfilter`进行注册挂接，但在调用其之前，需要填写一个`hook options`结构：
```c
struct nf_hook_ops {
	struct list_head list; //由内核管理
	// 用户只需填写下面内容即可
	nf_hookfn *hook; //自定义hook函数
	struct_module *owner;
	void *priv;
	u_int8_t pf; //协议族，例如：PF_INET,PF_ARP...
	unsigned int hooknum; //上面提到的五个hook点，需要注意：上述五个宏定义在/usr/include/linux/netfilter_ipv4.h 内核空间无法使用，所以需要自己定义
	int priority; //优先级
}
```

### libpcap
> [Libpcap 学习笔记](https://shaocheng.li/posts/2018/04/23/)
> 安装：`sudo apt install libpcap-dev`

其与`netfilter`包捕获机制不同，`libpcap`是在数据链路层增加一个旁路处理，而不是在内核网络协议栈中添加`hook`，其可将网络接口驱动层的数据包直接复制到用户空间的缓冲区中。

调用`libpcap`抓包的流程：
1. 查找可用网卡`pcap_lookupdev()`
2. 获取网卡参数`pcap_lookupnet()`获取指定网卡的IP地址和子网掩码
3. 打开网卡`pcap_open_live()`
4. 编辑过滤策略`pcap_complile()`
5. 设置过滤器，`pcap_setfilter()`将编译好的过滤策略设置到相应网卡上
6. 开始捕获数据包
7. 关闭网卡，释放资源

涉及的一些结构体定义：
```c
// 每一个网卡设备均可用该结构描述
struct pcap_if {
        struct pcap_if *next;
        char *name;             /* 网卡设备名称，可以调用 "pcap_open_live()" 打开*/
        char *description;      /* 网卡设备描述, 可以为 NULL */
        struct pcap_addr *addresses;  
        bpf_u_int32 flags;      /* PCAP_IF_ interface flags */
};
typedef struct pcap_if pcap_if_t;

struct pcap_addr {
        struct pcap_addr *next;
        struct sockaddr *addr;          /* address */
        struct sockaddr *netmask;       /* netmask for that address */
        struct sockaddr *broadaddr;     /* broadcast address for that address */
        struct sockaddr *dstaddr;       /* P2P destination address for that address */
};
typedef struct pcap_addr pcap_addr_t;

// 
struct pcap_pkthdr
{
  struct timeval ts;    /* 时间戳 */
  bpf_u_int32 caplen;   /* 本次捕获的数据长度 */
  bpf_u_int32 len;      /* 这个数据包的真实长度 */
};
```

相关函数：
```c
// 打开网卡
pcap_t *pcap_open_live(
	const char *device,  //网卡名称
	int snaplen, // 每个数据包，从开头抓取多少个字节，任何协议的数据包长度均小于65536
	int promisc, // 0->关闭混杂模式 1->打开~ （混杂模式：除了接受发往本机的数据包外，经过本机转发到其他主机的数据包也会被捕获）
	int to_ms, // 设置抓包函数的超时时间，0表示一直等待
	char *errbuf //存放错误信息
	);

// 连续捕获cnt个数据包后退出，若cnt<0，则持续捕获直至出现错误或调用pcap_breakloop
int pcap_loop(
	pcap_t * p,  
	int cnt, 
	pcap_handler callback, // 收到足够的数据包时调用该回调函数
	u_char * user //用户自定义数据，回调函数的第一个参数
	);
// 回调函数定义
typedef void (*pcap_handler)(
	u_char *user,  //见上文
	const struct pcap_pkthdr *h, // 捕获的数据包头信息 
	const u_char *bytes // 捕获数据包内容
	);
```

`libpcap`的过滤器是利用BPF（`Berkeley Packet Filter`）来筛选网络数据包的，它是类Unix系统上数据链路层的一种原始接口，过滤数据包需要完成三件事：
1. 构造过滤表达式，其为一个包含筛选规则的字符串
2. 将过滤表达式编译到BPF
3. 应用过滤器

过滤表达式语法：
1. 一个id
2. 多个修饰词
	1. `type`：指定id类型，例如：`net,host,port`，默认为host
	2. `dir`：指定id传输方向，例如：`src,dst`
	3. `proto`：指定匹配协议，例如：链路层协议(`ether`,`wlan`)，网络层协议(`IP`)，...
多个表达式之间可以使用`and or not`连接
```c
int pcap_compile(
	pcap_t *p, 
	struct bpf_program *fp, // 传出参数，保存编译好后的BPF
	const char *str,  // 过滤表达式
	int optimize, // 1表示对过滤表达式进行优化
	bpf_u_int32 netmask //设为0即可
	);
// 应用过滤器
int pcap_setfilter(pcap_t *p, struct bpf_program *fp);
```