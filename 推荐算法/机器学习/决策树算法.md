对于给定的一组数据集`{x,y}`，其中每个x均有一系列特征（也称之为属性）`A={a}`，决策树算法每次选择最优划分属性，将其划分到不同节点，在对这些不同节点重复上述操作。
1. 如果某节点的样本均属于某一类，则停止划分。
2. 如果某节点的样本属性均一致，则无法划分（此时可以取样本中占比最大的种类作为划分结果）

什么是最优划分属性？
1. 决策树希望每次对数据集进行划分时，分支节点包含的样本尽可能属于同一类别，也即结点的纯度越来越高。
2. 节点纯度可通过**信息熵**进行描述：$Entropy(D)=-\sum_{k=1}^{y} p_k log_2 p_k$，其中$p_k$表示节点样本集合D中第k类样本占比
3. 基于属性a进行划分时，假设划分为V个节点，则对于每个节点均可计算出对用的$Entropy(D_i)$，此次划分有**信息增益**为：$Gain(D,a)=Entropy(D)-\sum_{i=1}^{V}\frac{|D_i|}{|D|}Entropy(D_i)$
	1. 找到信息增益最大的属性a，便于最优划分属性。（**ID3算法**）
	2. ID3算法中，如果某属性划分的节点越多，求出的信息增益越大，为减少这种偏好带来的不良影响，**C4.5算法**提出了改进
		1. 使用**信息增益率**作为最优划分属性判别依据：$Gain_rate(D,a)=\frac{Gain(D,a)}{IV(a)}$
		2. $IV(a)=-\sum_{i=1}^{V}\frac{D_i}{D}log_2\frac{D_i}{D}$，相当于做了一次归一化
4. **CART算法**与上述两种完全不同，其采用**基尼值**描述节点纯度，采用**基尼系数**作为最优化分属性判别依据

### leaf/level-wise
决策树生成策略可划分为`leaf-wise`和`level-wise`两类，区别见下图：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410211821802.png)

---
开源库中`LightGBM`采用`leaf-wise`策略，`XGBoost`采用`level-wise`策略，区别主要在于每次划分最优属性时，`leaf-wise`在所有叶子节点上找到**一个**能够最大化降低损失函数的节点进行划分，`level-wise`则是**所有叶子节点均划分**。

`level-wise`易于进行多线程优化，但实际上比较低效，因为一些叶节点的分裂增益较低，没必要进行搜索和分裂。`leaf-wise`则解决了这一问题，缺点便是可能会长出比较深的决策树导致过拟合，为此需要添加一个最大深度的限制。

多线程优化：
1. 特征并行：不同特征在寻找最佳分裂点时使用不同的线程
2. 数据并行：将数据分割成多个子数据集，.....（不太理解，这样不会影响结果嘛？？？？）
### Histogram算法
> 直方图算法

决策树算法中，除了考虑不同属性的选择外，对于单个属性的划分界限也是值得考虑的问题，对于离散型特征，也不能简单粗暴按照每个子节点仅包含一个离散值进行划分。

直方图算法简单来看就是先对特征值进行分桶处理（直方图每个区间就是一个桶，称之为`bins`），在寻找最佳分裂点时，基于直方图计算，而不是在原始特征上进行计算，大大减少了需要考虑的特征值数量，从而提高训练速度。

### 其他优化
...