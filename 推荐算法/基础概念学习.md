> 1. [学习参考资料](https://datawhalechina.github.io/fun-rec/#/)
> 2. 《互联网大厂推荐算法实战--石塔西》
## 概述

推荐系统的意义：
1. 当前是一个信息过载的时代：
	1. 信息的生产者很难将信息呈现在对它们感兴趣的信息消费者面前
	2. 信息消费者也很难从海量的信息中找到自己感兴趣的信息
	3. **推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁**

搜索也可以解决上述问题，两者的区别是：
1. 搜索要求**用户意图**是明确的，推荐则是用户本身对自身需求不明确的手段
2. 搜索，由于用户限定了搜索词，所以个性化程度较低。推荐针对每个人不同的兴趣推荐相应的内容，**个性化程度**较高。
3. 优化目标不同，详见[排得更好VS估得更准VS搜的更全「推荐、广告、搜索」算法间到底有什么区别？](https://zhuanlan.zhihu.com/p/430431149)

## 架构设计

1. 系统架构：大数据背景下如何有效利用海量和实时数据
	1. 离线层：数据处理的主要部分，所有复杂操作均在此处完成，例如：
		1. `join`操作，埋点等仅记录了用户交互的商品ID，除此之外还需要关联查找商品表获取商品的详细信息
		2. ...
	2. 近线层：记录**准实时**数据，推荐领域的准实时数据一般是**用户行为数据**
	3. 在线层：**埋点**，记录实时用户行为（前后端的工作），为推荐系统收集数据
2. 算法架构
	1. **召回**：从推荐池中选择一定数量的item，送给后续的排序模块
		1. 要求：**不需要十分准确**的给出用户感兴趣的item，**但不能遗漏**
		2. 作用：缩小item范围，便于后续精排以可接受的时间消耗给出准确的结果
		3. 方式：多路召回，分为非个性化召回和个性化召回
	2. 粗排：介于召回和精排之间
		1. 不同于召回，粗排一般模型化
		2. ...
	3. **精排**：推荐算法的模型研究一般集中于该阶段
		1. 要求：对候选集中的item进行打分，**保证打分（优化目标）的准确性**
		2. 
	4. 重排：**业务层面优化推荐列表**
		1. 精排按打分生成TopN的推荐列表，可能相邻item之间同质化严重，直接展示给用户体验较差
		2. 重排根据一些业务策略对精排结果进行调整，例如：强制去重，流量扶持，多样性等等
	5. 混排：....


推荐系统的问题求解一般可看作：给定用户，推荐N个其最感兴趣的物品。

每次用户下拉刷新推荐页，都会走一遍`召回->重排`的流程，但是各个阶段的模型**一般离线训练，线上只做推理**（当然也存在在线训练的情况，但网上资料较少...），之前理解的时效性也是推理的耗时要低，而不是训练的耗时低（训练无论如何都不可能在秒级完成）。故有了`召回->精排`的链路，召回阶段的模型一般采用**后期融合**，可以提前推理出向量，线上通过最近邻查找解决时间问题。粗排和精排需要对物品进行打分，粗排涉及物品很多，所以不能使用过多的特征，不然推理速度太慢，而精排涉及的少，就可以较多的使用一些特征来打分（粗排和精排的模型一般是**前期融合**，只能线上实时推理）。

召回阶段：考虑到物品变化较小，其向量经**离线层**训练得到后保存起来，用户的兴趣变化较大，需要**在线层实时推理**得到用户向量，在通过最近邻查找得到召回池。（此处的模型为召回中常用的双塔模型）

精排阶段：


模型的训练及参数更新，一般分为两种方式：
1. 全量更新：每天凌晨，用昨天全天的数据训练模型更新参数
2. 增量更新：
	1. 目的：更新用户的ID Embedding，捕获用户当下的兴趣（不过用户行为序列等特征也可以反映用户当下的兴趣，这个作为出发点是否有必要呢？？）
	2. 实时收集数据，流式处理.....

上述过程在学习中，特别是竞赛里一般只关注精排阶段即可，学术界的研究也基本局限于此（可能个别比赛会涉及到召回）。

## 模型训练

### 排序学习
> 本质上是排序学习，也即Learning to Rank，简称LTR。
> Ranking是信息检索领域的基本问题，也是搜索引擎背后的重要组成模块，也即给定一个query，寻找与其相关的doc
> 参考资料：Learning to Rank for Information Retrieval（PDF） by LiuTY

相关性判别有三种策略：
1. `Relevance degree`：用相关度来数值化相关性。
	1. 例如：0/1表示relevant/irrelevant（二元分类），或者1/2/3/4/5表示相关程度Perfect，Excellent，Good，Fair，Bad
2. `Relevance preference`：用两个doc的相关关系来表示相关性。
	1. 例如：给定query，doc A比doc B更相关，样本表示为 doc A > doc B
3. `Total order`：给定query，其对应文档排序顺序作为相关性。
	1. 例如：给定query，排序为A，B，C...，排序关系本身包含了前两个的信息

机器学习框架的关键组件：
1. 输入空间：以特征向量表示的样本
2. 输出空间：样本对应的训练目标，也即label
3. 假设空间：从输入空间到输出空间的映射函数构成的空间，也即模型本身
4. 损失函数：不再过多解释

对应三种不同的训练方式：
1. `Pointwise`
	1. 输入空间：单个doc的特征向量
	2. 输出空间：取决于训练集，可以是doc与query的相关度，也即一个数值（也可以是该doc在query相关文档列表中的位置等等）
	3. 损失函数：取决于假设空间，模型可以采用回归，分类，序列回归等，对应损失函数略有不同
	4. 缺点：无论哪种训练方式，**由于损失函数独立看待每个doc，也即缺少了不同doc在最终排名列表中的位置信息**。
2. `Pairwise`
	1. 输入空间：一对（两个）doc的特征向量
	2. 输出空间：+1/-1，+1表示一对doc里前一个比后一个与query更相关，-1则相反
	3. 损失函数：
	4. 缺点：两两对比有时也很难推导出最终排名列表，仍然存在局限性
3. `Listwise`
	1. 输入空间：包含所有doc
	2. 输出空间：两种形式（query与doc列表中每一个doc的相关度量列表 或者 与query相关的doc列表）

排序学习的评估指标：
给定query，检索文档列表（待评估模型输出结果）：`#{document list}`，实际与query相关的文档列表：`#{relevant documents}`
举例说明：给定query，检索文档列表为：`R1,I1,R2,I2,R3,I4`，与query相关的文档为`R1,R2,R3`
1. P@K：给定query，`#{document list}`的前k个中存在n个相关文档，则其值为`n/k`
2. MAP（mean average precision）
	1. $AP=\frac{\sum_{k=1}^m P@K \cdot l_k}{\#\{relevant \space documents\}}$，其中m为`#{document list}`总数，**$l_k$是第k个位置文档与query相关性的二元判断**
	2. 举例的`AP=(1/1*1+1/2*0+2/3*1+2/4*0+3/5*1+3/6*0)/3`
	3. 所有测试query上AP的均值即为MAP
3. NDCG
	1. CG（Cumulative Gain，累计增益）：检索文档列表中各文档相关度类和
	2. DCG（Discounted CG，折损累计增益）：
		1. CG只关心累计相关度，但未考虑顺序，DCG则引入了位置折扣因$\frac{1}{log_2(r+1)}$，r为文档在检索列表的位置
		2. 除此之外，工业界常用的DCG公式为$\sum_{i=1}^p\frac{2^{l_i}-1}{log_2(i+1)}$
	3. NDCG（Normalized DCG，归一化折损累计增益）：
		1. 将DCG@K与其最大值（理想的排序）进行归一化可得到NDCG
		2. 归一化优势：不同query返回语料数量不同，DCG为累计值，无法互相比对，故归一化之后便于比较
4. MRR（mean reciprocal rank）平均倒数排名
	1. 给定query，模型输出文档序列中，第一个相关的文档位置记作`r(q)`，则MRR的值为$\frac{1}{r(q)}$
	2. 缺点：该指标仅考虑了第一个相关的文档，而`r(q)`之后的文档没有考虑
5. RC（Rank correlation）

### 推荐学习应用

推荐学习中各个链路的模型的训练也分为上述三种方式：
1. `Pointwise`：每次取一个样本（有正样本，也有负样本）
	1. 训练时，使得$cos(u,v^+)\to+1,cos(u,v^-)\to-1$
	2. **控制正负样本数量为1:2或1:3**
2. `Pairwise`：每次取一对样本（一个正样本，一个负样本）
	1. 训练时，使得$cos(u,v^+)尽可能大于cos(u,v^-)$，一般描述为$cos(u,v^+)-cos(u,v^-)-m$，m为超参数，一般设置为1
	3. 损失函数分为：
		1. `Triplet Hinge Loss`：$L(u,v^+,v^-)=max\{0,cos(u,v^-)+m-cos(u,v^+)\}$
		2. `Triplet Logistic Loss`：$L(a,b^+,b^-)=log(1+exp(\sigma(cos(a,b^-)-cos(a,b^+))))$
3. `Listwise`：每次取n个样本（`n>2`，其中一个正样本，`n-1`个负样本）
	1. 不同于LTR中的`listwise`，其定义是直接优化一个list，list中第i个doc要比第i+1个doc与query更相关
	2. 但此处显然没有这个要求，其本质上是n-1个`pairwise`
	3. 训练时计算用户向量u与n个物品向量的余弦，理想情况是正样本趋近于1，负样本趋近于0
	4. 可对n个样本与用户向量的余弦输入`softmax`函数，再计算**交叉熵损失**即可
---

正样本与负样本如何选择呢？首先明确一下什么是正样本。在推荐系统中，对于某个用户而言，其**感兴趣**的物品就是正样本，其不感兴趣的物品就是负样本。

正样本：曝光且点击的用户-物品二元组（当然不同场景下可能会有些微区别）
1. ...

负样本：
1. 召回模型筛掉的物品：**简单负样本**
	1. 未被召回的物品 约等于 全体物品，可直接在全体物品中进行抽样
	2. 如何抽样？由于二八法则，大部分物品都是冷门物品，均匀抽样对冷门物品不公平，需要非均匀抽样，抽样概率一般正比于其点击次数的0.75次方
	3. **Batch内负样本：也属于简单负样本**，与上述的区别是负样本在一个batch内选取，而不是全体物品中选取
		1. batch指一次模型训练时输入的n个样本，对应n个用户，n个正样本
		2. 我们可以把其余n-1个用户的正样本作为剩余用户的负样本进行训练
		3. 存在问题：正样本出现次数正比于其点击次数，此处的负样本选取也如此，会导致热门物品成为负样本的概率过大（按经验应该正比于0.75次方），打压过分
		4. 如何解决：训练模型时，将余弦相似度调整为$cos(u,v)-log(p)$，线上召回时使用正常的余弦相似度即可
2. 召回模型选中，但被粗排，精排筛掉的物品（未曝光）：**困难负样本**
	1. 之所以困难，是对于**召回模型而言，比较困难**
3. 粗排，精排选中，但用户未点击

实际训练<font color="#ff0000">召回模型</font>时，选择的负样本会混合简单负样本和困难负样本（一半一半）
但一定不要（**只**）把<font color="#ff0000">曝光未点击的样本作为召回的负样本</font>，这类样本是用于排序模型作为负样本。
原因在于召回和精排的目的不同：****
1. 召回：找到用户可能感兴趣的物品，只需要区分 感兴趣、不感兴趣
2. 精排：需要区分 很感兴趣，比较感兴趣

理论上的解释就是：离线训练时的数据分布，应与线上服务时的数据分布保持一致。
1. 召回模型：线上服务时的数据分布是面向全体物料池
2. 排序模型：线上服务时的数据分布是召回模型输出的用户可能感兴趣的物料池

## 特征工程

物料画像：
1. 物料自身属性：特殊的，ItemID在大厂也是重要的特征
2. 物料的类别与标签：
	1. 依赖于其他团队
	2. 例如：利用NLP技术分析物料的标题，摘要，评论等信息
	3. 例如：利用CV技术分析物料的封面，视频关键帧等信息
	4. 最终得出物料的类别与标签
3. 基于内容的Embedding：
	1. 不同于上一步给出类别和标签等稀疏向量
	2. 可直接把其他团队分析模型（例如CNN，Bert）某一层的输出作为类别内容标识（也即Embedding）
4. 动态画像：
	1. 上面提到的几类都是物料的静态画像，无用户交互
	2. 动态画像属于物料的后验统计数据，有偏差
5. 用户给物料反向打标签
	1. 例如：某物料经常被带有“娱乐”标签的用户访问，则该物料应该被打上“娱乐”的标签

用户画像：
1. 静态画像：性别，年龄等等
2. 动态画像：历史行为序列

## Embedding
> 石塔西认为推荐算法的经典问题是：“记忆”与“扩展”

所谓“记忆”，即记住常见高频模式，典型代表：LR模型。
举例说明：`<中国，饺子>与<美国，火鸡>`是较易出现的高频模式，模型会对这对组合设置较高的权重，而`<中国，火鸡>`则属于低频模式，模型的权重可能很低。

但推荐系统记住常见高频模式是很容易实现的，如何能为小众用户进行精准推荐才是系统的核心难点，为此就要求推荐算法具备”扩展性“，也即饺子，火鸡背后具备的**隐含语义**是：都与节日相关的食物。

人工拆解存在困难和局限，如何让模型自己挖掘一些特征的隐含语义呢？这就用到了`Embedding`技术。

`torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None,...)`
1. `num_embeddings`:词典中词的总数（离散特征中种类个数）
2. `embedding_dim`:转换之后embedding向量的维度
3. `padding_idx`: ????

前向传播：本质上是查表
1. `embedding.weight shape:(vocabulary size,embed_dim)`
	1. 如何查呢？通过`one hot+矩阵乘法`
2. `Input Shape:(batch_size,sequence_len`，其中`squence_len`可以为1
	1. `embedding(input)`的过程是先将`input`变成`(batch_size,sequence_len,vocabulary)`，再与`embedding.weights`矩阵相乘得到`(batch_size,sequence_len,vocabulary)`


Embedding的共享与独占问题：
1. 共享：
	1. 优势：解决数据量不足的问题
	2. 劣势：不同目标在训练期间相互干扰，个性化不足
2. 独占：需要较多的数据量

## 模型评估

推荐系统中一般的评估流程：
1. 离线评估：离线评估指标**仅能评估模型的优劣，但不代表业务会有提升**
2. 线上评估（AB实验）

排序算法的评估：
1. AUC：Area Under the Curve
	1. 真正例率（敏感性）：也即Recall查全率
	2. 假正例率（特异性）：反例（真反例，假正例）中假正例的占比
		1. 例如：过高会导致垃圾邮件分类模型中将正常邮件（反例）被标记为垃圾邮件，这是不可接受的
	3. ROC曲线：
		1. 如何绘制？模型对样本的预测值从大到小排序，并依次将其值作为阈值对样本进行分类划分，得到一组（Recall，FTR）的点并绘制在图上，阈值越高，Recall和FTR就越低，反之则越高
		2. ROC图中关键的点：（0，1），也即Recall=1，FTR=0，完美分类器，**所有的样本均分类正确**
		3. 同一模型根据ROC曲线选择阈值？
		4. 不同模型根据ROC曲线选择最优？
			1. 无交点：最外围，最靠近（0，1）点曲线对应的模型最优
			2. 有交点：取决于业务场景，看模型是看重召回率还是误判率
2. GAUC
3. NDCG

召回算法的评估：
1. Precision&Recall
	1. 二分类问题，可根据样本类别与模型预测类别分为：真正例，假正例，真反例，假反例
	2. Precision(查准率)：模型预测正例（真正例+假正例）中真正例的占比
		1. 区别于Accuracy（准确率），后者实际任务中意义不大，原因在于大部分任务在模型训练中，重点关注的样本较少的类别，例如癌症检测...
	3. Recall(查全率)：正例（真正例，假反例）中真正例的占比
	4. 上述两者一般互斥，所以有F1来综合考虑`Precision`和`Recall`
	5. PR曲线（较之ROC曲线）：
		1. **仅关注正例的分类精度，不关注反例的分类精度**，ROC比PR更关注模型的整体性能
		2. 
2. MAP
3. Hit Rate


值得一提的是，对于Pointwise训练的召回模型而言，其训练过程中在验证集测试时可以使用AUC来衡量模型本身二分类效果的优劣，但该召回模型在业务层面能否起到更好的效果需要关注上述`MAP`等指标...