## 前置知识

1. 什么是图`Graph`?
	1. 图用于表示一系列实体（`nodes`）之间的关系（`edges`）
	2. `GNN`中，图的每一个节点，每一条边均可用`embedding vector`表示，整个图也可以使用一个`embedding vector`表示
2. 数据如何表示成图？
	1. 图片（`w*h*channel`）：每个像素代表一个`node`，则非边缘位置的像素有8个邻居。每个`node`是一个`3-dims`向量，表示该像素的RGB值
	2. 文本（视作一个sequence）：分词后每个词作为一个`node`，文本便是一条有向边
	3. **社交网络**：每个用户作为一个`node`，存在关系的用户之间存在一条`edges`
	4. ...
3. 基于`Graph`数据，有哪些相关问题呢？
	1. `graph-level`：
	2. `node-level`
	3. `edge-level`
4. 如何表示图的**连接性**作为神经网络的输入？
	1. 邻接矩阵：节点过多时邻接矩阵过大
		1. 考虑到仅有少部分节点之间存在边，故可以使用稀疏矩阵进行存储，但高效的计算稀疏矩阵是比较困难的问题（尤其是在`GPU`上）
		2. 同一幅图，用邻接矩阵表示时，交换任意一行或列都不会改变图本身的含义，但是矩阵却发生变化
	2. 最终表示方案：
		1. 节点，边均使用一个标量表示
		2. 连接性则利用一个**邻接列表**表示，例如`[[1,0],[2,0]...]`表示节点1和节点0存在一条边，节点2和节点0存在一条边

## 网络结构

GNN是对图的各项属性（`node,edges,global-context`）进行一个可优化的变换（该变换可以保持图的对称信息）。其输入是一个`graph`，输出也是一个`graph`，在该过程中，图的各项属性都会变换，但是连接性不发生改变。

### MP-GNN
> Message Passing GNN，基于消息传递的图神经网络

1. 对于`graph`的`node,edges,global-context`对应的向量分别构造一个`MLP`对其进行变换。
	1. 很简单，弊端也很明显：在对图的各项信息进行`MLP`变换时并没有使用到图的结构信息（简单来说就是各个特征没有进行充分的交叉）。
2. 进一步：以`node`为例，更新时，将其与相邻`node`的向量做一次`pooling`，在丢入`MLP`中
	1. 这种处理类似于`Convolution`，不同的是此处对相邻`node`做`pooling`权重相等，卷积时每个像素权重对应于`kernel`里的元素
	2. 通过多层卷积，可以使得元素与更多元素发生交叉（也即`receptive Field`变大），类似，此处通过多层`MLP`，也可以使得`node`与更多的`node`进行交叉
3. 进一步：`node`不仅可以与相邻`node`进行`pooling`，也可以与相邻边（维度不一样的话先经过一次投影即可）进行一次`pooling`
	1. `node`与相邻`edges`汇聚后的向量再重新汇聚到`edges`上，与`edges`与相邻`node`汇聚后的向量再重新汇聚到`node`上是有区别的（谁更好不好说）
	2. `global-context`的意义：一些图可能边比较稀疏，导致某一个点想利用全局信息需要跨越比较深的网络，通过`global-context`则可以一开始就与全局信息进行交叉

什么是消息传递？
消息传递是GNN的核心机制，它描述了节点如何通过网络结构与其邻居节点交互信息。这个过程可以分解为三个关键函数：
1. 消息函数：节点如何向其邻居发送信息。
2. 聚合函数：所有邻居的消息合并为单一表示。
3. 更新函数：结合节点的当前状态和聚合后的消息来计算新的节点表示。

## GCN

GCN的消息传递函数：$H^{l+1}=f(H^{l},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{l}W^{l})$
各个符号的含义：
1. $H^{l+1}$：第l层的隐含层输出，维度为$N \times D$
	1. N为节点个数，D为节点的emb维度
2. A：邻接矩阵，表示图的结构
3. D：
4. W：权重矩阵

最简单的layer-wise propagation规则：$H^{l+1}=f(H^{l},A)=\sigma(AH^{l}W^{l})$
- $AH$的含义：将相邻的节点的信息相加得到自己下一层的输入
- 问题1：自身信息没了，解决方案：$\hat{A}=A+I$
- 问题2：经过一次的AH 矩阵变换后，得到的输出会变大，即特征向量 X 的scale会改变，在经过多层的变化之后，将和输入的scale差距越来越大，解决方案：A进行归一化
- 行归一化，也即每一行除以该行所有元素的和，表示为D，则为$D^{-1}A$
- 行+列归一化，表示为：$D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$
上述结合即可得到GCN的消息传递函数公式