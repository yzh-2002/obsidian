> CNN可有效处理空间信息，RNN则可以更好的处理**序列信息**

序列模型处理的问题可简化为：给定时间序列$x_{t-1},....x_{1}$，预测$x_t$，可通过估计条件概率$P(x_t|x_{t-1},....x_{1})$来预测。

第一个问题：随着时间的累计，已知时间序列会越来越长，如何处理历史时间序列呢？
1. 策略一：（隐马尔可夫）假设相当长的事件序列是不必要的，只需要距预测时刻长度为m的历史即可，也即$P(x_t|x_{t-1},....x_{t-m})$
2. 策略二：保留对历史时间序列的总结$h_t$，同时更新预测$x_t，h_t$，也即$P(x_t|h_t)$预测$x_t$，$g(h_{t-1},x_{t-1})$更新$h_t$

训练样本：$(x_{t-1},....x_{1}), x_t$
模型预测：k步预测，也即输入序列到$x_t$，但需要预测到$x_{t+k}$
1. 预测过程：先预测$x_{t+1}$，再构造输入序列$x_{t+1-m},...x_{t+1}$去预测$x_{t+2}$，直至预测到$x_{t+k}$
2. 存在问题：k越大，后续预测结果误差越大，原因在于每一步预测都带有误差，后面预测依赖于前面带有误差的输入，导致误差不断累计

## 语言模型
> 许多RNN的应用都是基于文本数据的，故选取语言模型，这一序列模型特例进行讲述

文本预处理：
1. 词元化（`tokenize`）：文本分词
2. 构建词表（`vocabulary`）：建立词与索引的对应关系（文本不易于训练）
3. `Corpus`：整个文本数据划分为词元后的索引列表
4. `stop words`：停用词，指英文中没有实际含义的虚词，例如`the,that,...`

语言模型：序列模型的一种，给定文本序列$x_1,....x_T$，目标是估计联合概率$P(x_1,....x_{T})$，联合概率可以利用乘法定理拆分为$P(x_1)P(x_1|x_2)...P(x_T|x_{T-1},...x_1)$，等同于上面序列模型的处理。

n元语法：类似于上文提到的隐马尔可夫假设，也即当前词的预测仅与过去n个词有关联。

如何针对`Corpus`划分出训练数据集？
![1782cdc1417dd9ba7ab6876d0f1f2375.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410092149268.png)

---
如何评价一个语言模型的好坏？
语言模型本质可以看作一个分类模型，也即给定前t-1个词元，预测第t个词元，其种类=词表大小。故可用交叉熵损失来衡量：$\pi =\frac{1}{n}\sum_{i=1}^{n}-log p(x_t|x_{t-1},...)$

由于历史原因，NLP使用困惑度来衡量，其值为$exp(\pi)$

## RNN原理

![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410092223161.png)

---
