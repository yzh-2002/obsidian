> CNN可有效处理空间信息，RNN则可以更好的处理**序列信息**

序列模型处理的问题可简化为：给定时间序列$x_{t-1},....x_{1}$，预测$x_t$，可通过估计条件概率$P(x_t|x_{t-1},....x_{1})$来预测。

第一个问题：随着时间的累计，已知时间序列会越来越长，如何处理历史时间序列呢？
1. 策略一：（隐马尔可夫）假设相当长的事件序列是不必要的，只需要距预测时刻长度为m的历史即可，也即$P(x_t|x_{t-1},....x_{t-m})$
2. 策略二：保留对历史时间序列的总结$h_t$，同时更新预测$x_t，h_t$，也即$P(x_t|h_t)$预测$x_t$，$g(h_{t-1},x_{t-1})$更新$h_t$

训练样本：$(x_{t-1},....x_{1}), x_t$
模型预测：k步预测，也即输入序列到$x_t$，但需要预测到$x_{t+k}$
1. 预测过程：先预测$x_{t+1}$，再构造输入序列$x_{t+1-m},...x_{t+1}$去预测$x_{t+2}$，直至预测到$x_{t+k}$
2. 存在问题：k越大，后续预测结果误差越大，原因在于每一步预测都带有误差，后面预测依赖于前面带有误差的输入，导致误差不断累计

## 语言模型
> 许多RNN的应用都是基于文本数据的，故选取语言模型，这一序列模型特例进行讲述

文本预处理：
1. 词元化（`tokenize`）：文本分词
2. 构建词表（`vocabulary`）：建立词与索引的对应关系（文本不易于训练）
3. `Corpus`：整个文本数据划分为词元后的索引列表
4. `stop words`：停用词，指英文中没有实际含义的虚词，例如`the,that,...`

语言模型：序列模型的一种，给定文本序列$x_1,....x_T$，目标是估计联合概率$P(x_1,....x_{T})$，联合概率可以利用乘法定理拆分为$P(x_1)P(x_1|x_2)...P(x_T|x_{T-1},...x_1)$，等同于上面序列模型的处理。

n元语法：类似于上文提到的隐马尔可夫假设，也即当前词的预测仅与过去n个词有关联。

如何针对`Corpus`划分出训练数据集？
![1782cdc1417dd9ba7ab6876d0f1f2375.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410092149268.png)

---
如何评价一个语言模型的好坏？
语言模型本质可以看作一个分类模型，也即给定前t-1个词元，预测第t个词元，其种类=词表大小。故可用交叉熵损失来衡量：$\pi =\frac{1}{n}\sum_{i=1}^{n}-log p(x_t|x_{t-1},...)$

由于历史原因，NLP使用困惑度来衡量，其值为$exp(\pi)$

## RNN原理

![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410092223161.png)

---
RNN基于时间反向传播

## GRU
> 模型提供了专门的机制来确定何时更新隐状态，何时重置隐状态
> 解决问题：
> 1. `SimpleRNN`中隐状态随着时间的累计，可能使得模型无法很好的对早期序列信息进行提取
> 2. `SimpleRNN`中隐状态是对所有序列信息的累计，包括了一些可能无用的序列信息
> 总之，GRU中的隐状态可以对序列中不同元素进行区分性的观测

1. 重置门（`R`）：计算和`SimpleRNN`隐状态一样，但是有单独的权重信息，以及激活函数选择`Sigmoid`
2. 更新门（`Z`）：同上
3. 候选隐状态：$tanh(X_t W_{xh}+(R_t \circ H_{t-1})W_{hh})+b_h$
	1. 与`SimpleRNN`隐状态更新区别于$R_t \circ H_{t-1}$
	2. 当R中元素趋近于0时，$R_t \circ H_{t-1}$也接近0，也即遗忘之前的序列信息
	3. 当R中元素趋近于1时，则与`SimpleRNN`一样
4. 最终隐状态：$Z_t \circ H_{t-1}+(1-Z_t)\circ H_t$，其中$H_t$为候选隐状态

GRU的训练过程与Simple RNN完全一致
## LSTM
> 效果和`GRU`基本一样，但是更加复杂

1. 输入门：计算和`SimpleRNN`的隐状态一样，但有单独的权重信息，以及激活函数选择`Sigmoid`
2. 遗忘门：同上
3. 输出门：同上
4. 候选记忆单元：同上，激活函数选择`tanh`，类似于候选隐状态
5. 记忆单元：类似于`GRU`的最终隐状态，$F_t \circ C_{t-1}+I_t\circ C_t$，其中$C_t$为候选隐状态
6. 最终隐状态：不同于`GRU`的地方在于其最终隐状态还要经输出门控制，$O_t \circ tanh(C_t)$

`LSTM`代码实现上与`SimpleRNN`，`GRU`显著不同的是，其`state`有两个，一个是最终隐状态，一个是记忆单元，这点需要注意。

## 其他RNN

1. 深度RNN：
	1. 不止一个隐含层，一个隐含层，其隐状态shape为`(batch_size, hidden_size)`，当有n个隐含层时，其隐状态shape为`(layer_size,...,...)`，且各层隐状态的权重独立
	2. 第l层隐状态$h_t^l$的更新依赖于$h_t^{l-1}$和$h_{t-1}^l$，也即某一时刻各层隐状态也许逐个计算，无法并行
	3. 输出层的计算仅依赖最后一个隐藏层的状态
2. 双向RNN
	1. ![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410111931114.png)
	2. 双向RNN一层隐含层相当于两层`Simple RNN`的隐含层
	3. 不同于单向，可以直接得到某时刻的输出，双向RNN需要正向，反向遍历整个序列之后，拼接正向和反向的隐状态再得到最终输出
		1. 虽然隐状态shape为`(batch_size, hidden_size)`，有两个（正向，反向）
		2. 但是遍历序列时需要记录对应时刻的隐状态，后续计算输出时使用

## Seq2seq
> 以`机器翻译`任务为例

`Seq2seq`遵循`Encoder-Decoder`架构。
1. `encoder`是一个模型，其将输入转换为一种中间状态
2. `decoder`也是一个模型，其接受`encoder`产生的中间状态，同时也接受输入，将其转换为我们最终想要的结果
3. 此处的中间状态可以简单理解为`RNN`中的隐状态。

具体而言，`Seq2seq`的`encoder`部分接受一个用户输入序列，并最终输出一个中间状态送入`decoder`中，`decoder`基于该中间状态+序列开始词元逐个生成后续词元。
1. `encoder`部分不做预测，仅输出一个中间状态，故可采用双向RNN
2. `decoder`部分做预测，故不能使用双向RNN

需要注意`decoder`在训练和推理时的不同：
1. 训练时，我们知道开始序列后的每一个词元，故此时不采用上一时刻的输出作为下一时刻的输入（也防止模型训练到后面错误累计，导致性能下降）
2. 推理时，不知道后续词元，只能使用...

机器翻译任务的预测序列如何评估？（区分于模型损失函数，这通常是两码事）

`BLEU`=$exp(min(0,1-\frac{len_{label}}{len_{pred}}))\prod_{n=1}^k p_n^{1/2^n}$

其中$p_n$是预测序列中所有`n-gram`的精度。其中$1-\frac{len_{label}}{len_{pred}}$是惩罚过短的预测，因为越短，其$p_n$精度越大，$p_n^{1/2^n}$是增加长匹配的权重，n越大，$1/2^n$越小，对于一个小于1的数，其指数越小，最终结果越大，也即权重越高。

### 束搜索

在机器翻译任务中，预测文本序列一般采用贪心策略，也即针对每个时刻我们选择预测概率最大的词，最终构成预测文本序列。

但每个时刻选择概率最大的词元不一定使得最终文本序列的条件概率最大，因为先前选的词会影响后续预测的词元概率，可能前面选了一个概率较小的词元，导致后续词元的最大概率都变大了。

基于此，我们可以使用穷举搜索，但词表大小为n，预测序列长度为T，则穷举的数量级为$n^T$，过大，难以落地。

束搜索：介于贪心和穷举之前，具体为：
1. 每时刻选择k个概率最大的，相邻两个选择有`k^2`个可能结果
2. 再从这`k^2`的可能结果中选择k个概率最大的
3. 最终时间复杂度为`nkT`
