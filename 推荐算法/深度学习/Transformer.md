## Attention
目前网上给出的`attention`的引入思路都类似于`KNN`，也即$f(query)=\sum_{i}^{n} \alpha(key_i,query)\cdot value_i$，也即`query,key`为样本，`value`为样本对应标签。

但提到`attention`的优势，无一例外在讲

![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410121846779.png)

---


## Self-Attention
