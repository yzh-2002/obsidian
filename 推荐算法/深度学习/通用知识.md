## 反向传播

![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202411022115693.png)


如上图所示：
**勘误：**
图中最后一行求和式第一项分母应该为：$a_k^{l-1}$，下标写错了

$\frac{\partial{C}}{\partial{w^{l}_{jk}}}=\frac{\partial{z^l_j}}{\partial{w^{l}_{jk}}}\frac{\partial{a^l_j}}{\partial{z^{l}_j}}\frac{\partial{C}}{\partial{a^{l}_j}}=a^{l-1}_k\sigma^{'}(z^l_j)\frac{\partial{C}}{\partial{a^{l}_j}}$，$\frac{\partial{C}}{\partial{b^{l}}}$略，只是第一项为1即可
关键便是$\frac{\partial{C}}{\partial{a^{l}_j}}$的求解：
当`l=L`时，此处$a_j^l$为模型输出层，直接根据Loss定义求导即可
当$l\neq L$时，需要反向传播，一层一层向后求导，也即$\frac{\partial{C}}{\partial{a^{l}_j}}=\sum_{j=0}^{n}w_{jk}^{l+1}\sigma^{'}(z^{l+1}_j)\frac{\partial{C}}{\partial{a^{l+1}_j}}$
上述表达式转换为矩阵形式：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202411031353152.png)
......

---

## 梯度消失/爆炸

![c77bcf6f33e7a9b40a29e7ab70982cda.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202411031353710.png)
......

---
梯度在合理的范围内，一般是`1e-6,1e3`。
梯度消失：
1. 梯度过小，无法训练 or 输出层各层梯度不算太小，能对参数进行训练，但由于梯度累乘，使得输入侧各层的梯度消失，无法训练，最终模型与浅层一样，无法发挥其优势
2. 解决办法：
	1. 避免使用`Sigmoid`激活函数
	2. 添加加法，例如：ResNet，LSTM

梯度爆炸：
1. 最糟糕的情况：超出值域，NaN
2. 学习率过于敏感：如果学习率设置的稍微大了一点，由于梯度爆炸，会使得参数值更新过大，而W过大，又会使得梯度进一步膨胀。
3. 解决办法：
	1. 梯度裁剪：$g \to min(1,\frac{\theta}{||g||})g$
		1. g：模型所有参数的梯度
		2. 梯度裁剪保证其`L2 Norm`在$\theta$之内

通用的解决办法：合理的权重初始化...