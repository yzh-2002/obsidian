## 特征交叉模型
> 深度学习之前，精排环节主要依赖于`Logistic Regression`

推荐系统中的LR模型（及其他所有模型）需要满足以下两条技术要求：
1. 在线学习（SGD天然支持增量更新参数，当一次只输入一条样本时变成OGD）
2. 模型参数尽可能稀疏
	1. （通用）降低模型的复杂性，防止过拟合
	2. 推荐系统的特征往往是高维稀疏的，（对LR而言）每个特征对应一个权重会导致存储，查询的性能压力较大，解决方案：如果某个特征不重要，直接将其权重置为0，而不是保留一个非常接近0的小数

但OGD根据单一样本计算的梯度，随机性较大，使得L1正则化（在批量训练时效果较好）难以压缩特征产生足够的稀疏解，为解决该难题，Google提出了`FTRL`算法。

### FTRL

### FM
> 前身是LR，除此之外，**手动引入了二阶特征交叉**

$logit_{FM}=b+\sum_{i=1}^{n}w_i x_i+\sum_{i=1}^{n}\sum_{j=i+1}^{n}w_{ij}x_i x_j$

存在问题：
1. 假设共n个特征，二阶交叉项引入了$n^2$个参数，需要更多的数据，否则会过拟合
2. $w_{ij}$要想得到训练，则要求$x_i,x_j$均不为0的样本足够多，但推荐系统中的类别特征高维稀疏，故符合条件的样本很少，无法充分训练（tip：此时未引入Embedding）
	1. 梯度下降时，损失函数关于$w_{ij}$的偏导为$x_i x_j$，若其均为0，则偏导为0，自然得不到训练

$logit_{FM}=b+\sum_{i=1}^{n}w_i x_i+\sum_{i=1}^{n}\sum_{j=i+1}^{n}(v_i\cdot v_j)x_i x_j$

模型对于每个特征，参数除了一阶权重$w_i$，还包括一个Embedding向量，也即$v_i$，而特征的二阶权重则为对应两个交叉特征的emb向量点积，其优势：
1. 参数量由$n^2 \to nk$，其中k为emb向量维度，大大减少学习参数量
2. 如下推导可知，只要$x_i\neq 0$，便可训练$v_i$，也便间接训练了$w_{ij}$，数据利用率更高，训练更加充分
3. 除了训练上更优越，还提升了模型的扩展性，原始的特征交叉，如果某种$x_i x_j$未在样本中**一起出现**，则无法预测，但更改后的，只要$x_i$和$x_j$在样本中单独出现，便可以训练对应隐向量
![36a24a41cc61a98a689dd7f06f1bab0a_720.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410071351470.png)
---

### Wide & Deep
> 

![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/notes/202410071951357.png)

---
上图从左到右依次展示了三类模型：`Wide Models,Wide & Deep Models,Deep Models`
`Wide & Deep Models`分为两部分：
1. `Wide侧`：LR
	1. 优势：**强于记忆**，把在训练数据中**高频，大众**的模式牢牢记住
2. `Deep侧`：遵循推荐模型的经典设计范式（`Embedding + MLP`）
	1. 优势：利用`Embedding`扩展特征内涵，再使用DNN对特征进行高阶隐式交叉，大大增强模型的扩展性，满足**低频，小众**的用户需求

### FM & Deep (DeepFM)


### Cross & Deep (DCN)
> 显式交叉 vs 隐式交叉
> DNN，万能函数模拟器，也即理论上只要网络层数够多，每层足够宽可以模拟任何函数，但实际实验中，其可能连2阶或3阶特征交叉都模拟不好
> 故推荐领域模型设计出现一种思路：**实现特征交叉，仅靠DNN的隐式交叉是远远不够的，还应该添加显式，指定阶数的交叉作为补充**
> 1. `Wide & Deep`使用一阶LR作为Deep侧补充
> 2. `DeepFM`使用FM二阶交叉作为Deep侧补充
> 3. `DCN`则可任意指定阶数的显示交叉，补充Deep侧



### AutoInt
> 同样出于对DNN交叉能力的不满，`AutoInt`借助`Transformer`结构实现特征交叉

## 序列模型
> 用户行为序列，是推荐系统中最重要的一类特征，其蕴含着丰富的用户兴趣信息

1. 行为序列信息的构成（以`用户最近观看的50个视频`为例说明）
	1. 基本元素：每个视频ID的embedding向量
	2. 时间差信息：每个视频距本次请求时刻之间的时间差（桶化为整数再Embedding）
	3. 元素元信息：视频的作业...
2. 简单Pooling：将用户的行为序列压缩为一个兴趣向量
	1. `Sum Pooling`
	2. `Average Pooling`
	3. `Weighted-sum Pooling`：权重可以是时间差或动作程度
3. 复杂Pooling：
	1. 借助`Attention`的思想，简单Pooling的问题：将序列中所有元素一视同仁。但实际中，不同历史记忆对当下决策的影响程度不同。
	2. ..
### DIN
> Deep Interest Network

原理很简单，就是将候选item作为query，用户历史序列作为key-value进行attention，从而将用户历史行为序列压缩成一个根据候选物料变化的稠密向量（然后交给上层模型参与CTR建模）。

上述存在一个不足之处在于：只刻画了候选物料与历史行为序列之间的交叉，却忽略了用户历史行为序列中各元素之间的依赖关系。例如：如果一个用户购买过`MacBook`和`iPad`，这是一个非常强烈的信号，说明用户是个忠实的苹果用户。

解决办法也很简单，在候选item与历史行为序列进行attention之前，历史行为序列先进行一次`self-attention`，也即最终模型是双层`Attention`架构。

### SIM
> Search-based Interest Model

`Attention`机制的一大缺点：当用户历史行为序列比较长时，计算复杂度较大。但推荐系统希望尽可能对用户长期行为序列建模，因为如果建模序列太短，难免会包含一些用户临时起意的行为，算是一种噪声。

`SIM`提出了一种解决方案：
先对用户历史行为序列做一次过滤（搜索），挑选出与候选物料相关的一个短序列，再进行DIN建模
1. 硬搜索：通过物料的分类（或标签）过滤
2. 软搜索：`Item embedding`近邻查找

## 多任务学习
> 多任务学习的模型结构是当前精排模块的业界主流
> 原因在于：很多推荐业务，天然就是多目标的建模场景

以电商场景为例，建模目标有以下三个：
1. 从曝光到点击，点击率（CTR）
2. 从点击到购买，转化率（CVR）
3. 从曝光到购买，CTCVR

1. 为什么不为每个目标单独建模？
	1. 每个推荐模型都消耗大量内存，算力，单独建模损耗更大
	2. 用户转化是一个链条，越后面可用于训练的正样本越少，单一训练对于靠后环节的模型训练效果较差
		1. 多任务学习可以利用前面数据多的环节向后面数据少的环节做“知识迁移”
2. 为什么不直接对终极目标建模？
	1. 正样本少，难以训练
	2. **点击率（等指标）也很关键**，用户没购买不代表不喜欢，可能只是当前商品价格过高，如果仅关注CTCVR，则可能会使得模型推给用户的都是在其消费能力之内的中低端商品
		1. 危害一：用户审美疲劳，不利于用户的长期留存
		2. 危害二：如果哪天用户想消费了，但模型未能给他推荐喜欢的高端商品，则浪费了机会
### 并发建模

> 每个目标单独建模，忽略不同目标之间的因故关系

#### Share Bottom

底层的`embedding`层和DNN层为所有任务共享，上层则为不同任务设立不同的模型结构，其输入均来自共享底层输出。
1. 优点：实现多任务间的知识迁移，电商场景下，`clicks`样本数量很多，`carts/orders`数量很少，通过共享，可以使得每种操作涉及的`item`的均被训练的很好
2. 缺点：
	1. 负迁移：任务A与B的联合训练结果，任务A的效果比单独训练A效果差，任务B也如此
	2. 跷跷板：~，任务A比单独训练好，但任务B比单独训练差
#### MMOE
> Multi-gate Mixture-of-Experts

讲`shared bottom`拆分为若干小型DNN，每个DNN可以称为一个`Expert`，再由一个门控网络控制每个`Expert`对某个任务的参与程度。
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/cs/202410282101869.png)

---
第k个目标的输出为：$y_k =h_k(\sum_{i=1}^{n}g_k(x)_iExpert_i(x))$
1. $h_k$：第k个目标的上层模型结构
2. $g_k(x)_i$：第k个目标的门控网络针对第i个专家网络的权重
	1. 具体实现上，门控网络是一个普通的`MLP`，最后一层使用`Softmax`使得各`Expert`的权重之和为1

后续发展：
1. `PLE(Progressive Layered Extraction)`：
	1. `MMoE`中所有`Expert`为所有任务共享，`PLE`中则分为两大类：前者只参与单一任务，后者参与所有任务
	2. `PLE`引入了多层`Expert`
### 串行建模
> 主要用于电商场景，其中`CTR,CTCVR`使用并发建模也可以，但是`CVR`就存在问题

`CVR`是从点击到购买的概率，字面来看，需要使用点击数据来训练，但预测时，需要对尚未曝光的物料预测包括CVR在内的分数，这样就会导致训练和预测时物料分布不一致，导致样本选择误差（`Sample Selection Bias SSB`）

1. ESMM：
	1. 消除SSB，将`CVR`作为隐藏目标，在其他目标被优化的同时间接被优化
2. ESM2
	1. 点击与购买之间有多种中间行为也是强力信号（加入购物车，收藏，无行为）
3. ...

