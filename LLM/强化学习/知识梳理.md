> 参考资料：
> 1. [# 强化学习（Reinforcement Learning）知识整理](https://zhuanlan.zhihu.com/p/25319023)
> 2. [# 零基础学习强化学习算法：ppo](https://www.bilibili.com/video/BV1iz421h7gb)
> 3. 

MDPs （Markov Decision Processes）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。
MDP 的策略<font color="#ff0000">完全取决于当前状态</font>（Only present matters），这也是它马尔可夫性质的体现。

符号表示：
1. $s \in S$：S表示有限状态集合，s表示某个特定状态
2. $a \in A$：
3. $T(S,a,S^{'})\sim P_r(s^{'}|s,a)$：
4. $R(s,a)$： agent 采取动作a后的**期望**<font color="#ff0000">奖励</font>
5. $Policy~ \pi(s) \to a$：输入state，输出action的概率分布
6. $U(s_0,s_1,s_2,...)=\sum_{t=0}^{\infty}\gamma^tR(s_t)\leq \sum_{t=0}^{\infty}\gamma^tR_{max}=\frac{R_{max}}{1-\gamma}$：<font color="#ff0000">回报</font>，也记作$G$，表示从某个时间开始，未来所有奖励的累加
	1. $\gamma$：折损率
7. $v(s)=E[U_t|S_t=s]$（状态价值函数）：基于 t 时刻的状态 s 能获得的未来回报（return）的期望
	1. $U_t=\sum_{i=t}^{T-1}\gamma^{i-t}R(s_{i+1})$
8. $q_\pi=E_{\pi}[U_t|S_t=s,A_t=a]$（动作价值函数）：基于 t 时刻的状态 s，选择一个 action 后能获得的未来回报（return）的期望
9. $\{s_0,a_0,s_1,a_1,...\}$：Trajectory，轨迹，一连串状态和动作的序列

## Bellman方程

> RL的目标：训练一个Policy神经网络$\pi$，在所有状态s下，给出相应的action，得到Return的期望最大
> - 区分Reward和Return

![](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/blog/20250603203523358.png)

---

$v_{\pi}(s)=E[U_t|S_t=s]=\sum_{a\in A}\pi(a|s)(R(s,a)+\sum_{s^{'}\in S}P(s,a,s^{'})v_{\pi}(s^{'}))$
- 对于确定性环境（例如：国际象棋），从状态s执行动作a，下一时刻状态确定
- 对于随机性环境（例如：自动驾驶仿真环境），从状态s执行动作a，<font color="#ff0000">下一时刻状态不确定</font>，上述公式即是这种环境
- $P_{ss^{'}}=P(s,a,s^{'})=P(S_{t+1}=s^{'}|S_t=s,A_t=a)$
- 将概率转换为期望：$v_{\pi}(s)=E_{\pi}[R(s,a)+\gamma v_{\pi}(S_{t+1})|S_t=t]$

上述公式可以写成如下矩阵形式：
![](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/blog/20250603170522208.png)
也即：$v=R+\gamma Pv \to v =(1-\gamma P)^{-1}R$，时间复杂度为$O(n^3)$

---
动作价值函数同理可得：$q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]$
- 区分$R_{t+1}和R(s,a)$
	- 前者：在时间步t执行动作a后，环境在t+1时刻返回的实际奖励，对于随机性环境而言是不确定的（$S_{t+1}$不确定）
	- 后者：在给定状态s下采取动作a后的期望奖励（$R(s,a)=E[R_{t+1}|S_t=s,A_t=a]$）。
		- <font color="#ff0000">WARNING</font>：实际情况下$R_{t+1}$不仅与$S_t，A_t$有关，还和$S_{t+1}$有关，但有时为了简化模型，常常不依赖$S_{t+1}$

两者的关系（参考上图）：
$v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)=E[q_{\pi}(s,a)|S_t=s]$
$q_{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{'} \in S}P_{ss^{'}}^av_{\pi}(s^{'})$，此处的$R(s,a)$和最上面$v_{\pi}(s)$中的不是同一个，此处的a为固定值。

最优方程：
$v_{*}(s)=max_{\pi}v_{\pi}(s)$
$q_{*}(s,a)=max_{\pi}q_{\pi}(s,a)$

求解Bellman最优方程：
1. Model-based：学习Transition Model T(S,a,S′)∼Pr(s′|s,a)，即在状态 s 采取行动 a 后转移到 s' 的概率，然后基于这个 Model 去选择最优的策略。
	1. Policy Iteration
	2. Value Iteration
2. Model-free：未知 Transition Model，通常通过不断的尝试去直接学习最优策略。
	1. Temporal-Difference（TD）学习
		1. Q-learning
		2. SARSA
	2. 策略梯度学习
		1. PPO
		2. ...

## Model-based

### Policy Iteration

1. Iterative Policy Evaluation：

## Model-free

### Q-learning

### SARSA
> State-Action-Reward-State-Action

