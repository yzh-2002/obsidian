> 参考资料：
> 1. [# 强化学习（Reinforcement Learning）知识整理](https://zhuanlan.zhihu.com/p/25319023)
> 2. [# 15 增强学习101 闪电入门](https://nooverfit.com/wp/15-%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0101-%E9%97%AA%E7%94%B5%E5%85%A5%E9%97%A8-reinforcement-learning/)
> 3. 

MDPs （Markov Decision Processes）简单说就是一个智能体（Agent）采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境（Environment）发生交互的循环过程。
MDP 的策略<font color="#ff0000">完全取决于当前状态</font>（Only present matters），这也是它马尔可夫性质的体现。

RL的本质是学习从环境状态到动作的映射（即行为策略），记为策略$\pi: S \to A$。而仅仅使用立即回报r(s,a)肯定是不够的(一个策略π的长期影响才是至关重要的).

符号表示：
1. $s \in S$：S表示有限状态集合，s表示某个特定状态
2. $a \in A$：
3. $T(S,a,S^{'})\sim P_r(s^{'}|s,a)$：
4. $R(s,a)=E[R_{t+1}|s,a]$： agent 采取动作a后的即时奖励
5. $Policy~ \pi(s) \to a$：根据当前 state 来产生 action
6. $U(s_0,s_1,s_2,...)=\sum_{t=0}^{\infty}\gamma^tR(s_t)\leq \sum_{t=0}^{\infty}\gamma^tR_{max}=\frac{R_{max}}{1-\gamma}$
7. 