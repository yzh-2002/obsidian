1. Nvdia
	1. A100（40/80 GB）（企业级）
	2. H100（80 GB）
	3. V100（16/32 GB）
	4. RTX 4090（24 GB）（消费级）
		1. [# 4090 魔改 48g 显存是怎么做到的？](https://www.zhihu.com/question/11803840385)
2. Google TPU：专为Tensorflow优化，性能高，仅限云端使用，不可购买实体

目前大模型参数量xxB，B指billion（十亿，约等于1GB，也即1024MB，`1024*1024`KB，`1024*1024*1024`B（Byte））
故评估大模型所需显存只需要知道一个参数多少字节即可。
以FP16精度为例，浮点数16位，也即2字节，则xB的大模型需要2xGB的显存

训练时，常采用FP32/16精度，推理阶段常采用FP16/INT8/INT4来加速