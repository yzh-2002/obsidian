> 目标：研二在学校从事相关工作，能发一两篇论文（研二开学前应对大模型各个技术有较为深刻的学习和理解）

### Q：如何从零开始训练大模型？

> 背景：当下不少工作都是选择一个较强的基座模型进行微调，但实际情况下会遇到问题
> 基座模型主要存在的问题：语言不匹配+专业知识不足


1. 预训练阶段（该过程是训练模型<font color="#ff0000">续写</font>的能力）
	1. Tokenizer Training
		1. WordPiece
		2. Byte-level BPE
		3. 词表扩充
	2. Language Model PreTraining
		1. 数据源采样
		2. 数据预处理：将「文档」进行向量化
		3. 模型结构
		4. Warmup & Learning Ratio 设置
	3. 数据集清理
	4. 模型效果评测
2. <font color="#ff0000">SFT</font>，指令微调阶段（模型可以续写，但无法恰如其分的回答问题）
	1. Self Instruction，训练数据包含三部分：
		1. instruction 代表要求模型做的任务
		2. input 代表用户输入
		3. output 代表喂给模型的 label
	2. Instruction Tuning Dataset
		1. Alpaca
		2. BELLE
		3. ...
	3. 模型评测
3. 奖励模型
4. 强化学习
	1. Best-of-N（BON）
	2. Direct Preference Optimisatio（DPO）
	3. Proximal Policy Optimization（PPO）


### Q：Deepseek的改进之处？
