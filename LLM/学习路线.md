> 目标：研二在学校从事相关工作，能发一两篇论文（研二开学前应对大模型各个技术有较为深刻的学习和理解） 

## LLMs

大模型可学习参数的数量级如何计算？
大模型中参数主要来源：
1. Embedding：
	1. 输入是vocabulary大小V，输出为隐含层单元个数H
	2. 参数量：VH
2. Transformer块（L个）：
	1. Multi-Head Attention：N个head
		1. q，k，v会先经过一个投影矩阵，其输入为H，输出为H/N
		2. 最后会concat再经过一个MLP
		3. 参数量：$H*\frac{H}{N}*3*N+H^2=4H^2$
	2. FFN：两层MLP
		1. 第一层的输出通常为4H
		2. 参数量：$H^2*2=8H^2$

总共参数量：$VH+12H^2L$

### GPT
>Generative Pre-Training
><font color="#ff0000">只包含Transformer的Decoder部分</font>，预训练语言模型

时间线：
1. 2017/06 transformer
2. 2018/06 GPT
3. 2018/10 Bert
4. 2019/02 GPT-2
5. 2020/05 GPT-3
6. 2022/03 Instruct GPT
7. 2023/03 GPT-4

GPT1：

预训练：
1. 训练方式：self-supervised learning（无监督学习的一种，利用数据本身的结构或信息来生成监督信号，不需要人工标注的标签。）
2. 目标函数：与传统的语言模型一致（自回归语言建模）
---
微调：
1. 训练方式：supervised learning

GPT2：
zero-shot如何实现？
在GPT1中，为了应对各种下游任务，需要进行各种微调，如何在不微调的情况下能解决各种各样的下游任务呢？<font color="#ff0000">prompt</font>

Instruct GPT：
![image.png](https://raw.githubusercontent.com/yzh-2002/img-hosting/main/cs/202505041947489.png)



---

GPT4：技术报告中没有任何技术细节


### Bert
> Bidirectional Encoder Representations from Transformer
> <font color="#ff0000">只包含Transformer的Encoder部分</font>，预训练语言模型

预训练语言模型的应用：
1. 基于特征：将预训练语言模型输出的Emb作为特征输入指定任务的模型中
	1. 例如：ELMo（双向信息，RNN结构）
2. 基于微调：针对指定任务，对预训练模型进行微调
	1. 例如：GPT（单向信息，transfomer结构）

输入为一个序列，且序列最开始的token是[CLS]（Bert希望该token最后的输出代表整个序列的信息），Bert中每个token的embedding表示有三部分组成：token/segment/position embedding

目标函数：
1. MLM（masked Language model）：随机遮盖输入中15%的词汇，模型基于双向上下文预测被遮盖的词（交叉熵损失）。
2. NSP（Next Sentence Prediction，非必须）：判断两个句子是否为连续文本（二分类损失）。


### Llama
> 2024/07 Llama 3.1

Pre-Training: 
1. xxx
2. Scaling Law



### GLM


## 从零开始训练大模型

### PreTrain
><font color="#ff0000"> 预训练阶段：让模型积累知识储备</font>
> 1. 从零开始预训练大模型（模型参数随机初始化），需要大量的训练数据+众多高显存的显卡资源
> 2. 在开源的预训练大模型基础上，针对特定的专业领域，提供对应训练数据进行训练，使模型获取专业领域的知识（最常见的情况）

#### Tokenizer Training

首先，我们会选择一个开源的预训练模型（称之为基座），但开源模型基本都使用英文语料进行预训练，对于中文支持度不好。

所以，需要将基座在中文预料上进行二次预训练。在正式的预训练之前，需要先做一件事：词表扩充。

大模型在训练时，会将预料拆分为一系列的token，其任务就是给定一个token，去预测下一个token。词表扩充则是，对于中文预料，如何拆分成一系列token呢？
1. WordPiece：很简单，将常用字和常用词均存储到词表里，遇到某个字或词不存在词表中时（Out of Vocabulary），tokenizer将其标记为[UNK]
2. Byte-level BPE（BBPE）：[参考链接](https://zhuanlan.zhihu.com/p/146114164)
#### Language Model PreTraining

1. 数据源采样：
	1. 对于规模较小的数据集，会被多训练几次（相当于过采样）
	2. 使得模型不会太偏向于规模较大的数据集，从而失去对规模小但作用大的数据集上的学习信息。
2. 数据预处理：如何将「文档」进行向量化。
3. 模型结构：为了加快模型的训练速度，通常会在 decoder 模型中加入一些 tricks 来缩短模型训练周期。
	1. Attention（MQA，Flash Attention）
	2. Position Embedding（ALiBi，RoPE）
4. Warmup & Learning Ratio 设置

#### 数据集清理
> 精心清理的互联网数据（Common Crawl）比精心构造的数据（Arxiv，wikipedia）有更好的效果
> 
> <font color="#ff0000">数据比模型结构重要，恰恰说明transformer结构更好，具有非常强的学习能力，目前仍然没有完全达到它的极限</font>

如何清洗？
1. 语言过滤
2. 规则过滤
3. 模型过滤：训练一个轻量的机器学习模型，用于判断一篇文章是否足够优质。
4. 文章去重

#### 模型效果评测

1. PPL：Perplexity，$PPL=2^{H(P,Q)}$
2. BPC：Bits-per-character，$BPC=\frac{1}{T}\sum_{t}^{T}H(P,Q)$
	1. CE在语料长度上的均值
	2. 上述两个指标只能判断模型是否能生成通顺的下文，但现在LLM基本都具备这个能力，现在更重要的指标是其<font color="#ff0000">知识蕴含能力</font>
3. C-Eval（做题）

### Instruction Tuning Stage
> 预训练任务的本质在于「续写」，而「续写」的方式并一定能够很好的回答用户的问题（也不一定是回答问题，而是<font color="#ff0000">指令对齐</font>）。

#### SFT
> Supervised Fine tuning

1. 全参数微调
2. 部分参数微调
	1. peft（Parameter-Efficient Fine-Tuning）
		1. LoRA（Low-Rank Adaptation）
		2. Adapter

#### Self Instruction

既然我们需要去「教会模型说人话」，那么我们就需要去精心编写各式各样人们在对话中可能询问的问题，以及问题的答案。

OpenAI在这部分花费了大量的财力进行人工标注（SFT，监督微调）。但后续，我们可以通过 ChatGPT 的输入输出来蒸馏（Distill）自己的模型。

数据获取：先验指令+种子数据 => GPT
数据评估：也可交给GPT来做（需要设置Prompt）

#### 开源数据集

示例：
1. instruction 代表要求模型做的任务
2. input 代表用户输入
3. output 代表喂给模型的 label。
```json
{
    "instruction": "Arrange the words in the given sentence to form a grammatically correct sentence.",
    "input": "quickly the brown fox jumped",
    "output": "The quick brown fox jumped quickly."
}
```


1. Alpaca
2. BELLE
3. Vicuna
4. BAIZE

#### 模型效果评测
略

### Reward Model
> RLFH reinforcement learning from human feedback

1. 给定Question，有多个结果，人工对结果好坏进行排序
	1. 用「相对任务」替代「绝对任务」能够更方便标注员打出统一的标注结果。
	2. 便于模型的学习
2. Rank Loss

#### RM的必要性

**我们一直都在告诉模型什么是「好」的数据，却没有给出「不好」的数据**。

SFT 的目的只是将 Pretrained Model 中的知识给引导出来的一种手段，而在SFT 数据有限的情况下，我们对模型的「引导能力」就是有限的。这将导致预训练模型中原先「错误」或「有害」的知识没能在 SFT 数据中被纠正，从而出现「有害性」或「幻觉」的问题。

```json
{
    "prompt": "下面是一条正面的评论：",
    "selected": "屯了一大堆，今年过年的话蛮富足的!到货很快的!",
    "rejected": "对商品谈不上满意，但是你们店的信誉极度不满意，买了件衣服取消了订单，要求退款，结果退款过程进行一半就不进行了，真是因小失大啊"
}
```

### 强化学习
> 在获得了一个 Reward Model 后，我们便可以利用这个 RM 来进化我们的大模型。

#### BON
> Best of N

1. 通过设置 temperature 值让同一个模型生成若干回复
2. 使用 Reward Model 挑出这些回复中得分较高的回复并再次训练原本的模型

#### DPO
> Direct Preference Optimisatio，直接偏好优化
> 一种不需要 Reward Model 的训练方法，它可以用训练 RM 的偏序对来直接训练模型本身。

DPO 借鉴了对比学习的思路，
其目标是：**对于同一个 prompt，尽可能大的拉开 selected 答案和 rejected 答案之间的生成概率。**

#### PPO
> Proximal Policy Optimization，近端策略优化



## LLM Application


### Agent

### RAG

