
## Norm

Q：为什么需要Norm？
Q：BatchNorm？LayerNorm？RMSNorm？
Q：区别？


## Position Encoding

1. 绝对位置编码
	1. Sinusoidal 位置编码（原始Transformer）
		1. 对于第pos个token，其位置编码向量PE可表示为：$PE_{(pos,2i)}=sin(\frac{pos}{10000^{2i/d}}),PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{2i/d}})$
		2. 可类比于二进制编码来理解这种编码方式的绝对性，参考[[MiniMind]]
	2. 可学习位置编码（Bert，GPT） 
		1. TODO：如何有效的学习位置？
2. 相对位置编码：关注序列中元素之间的相对距离，而非绝对位置
	1. ALiBi（Attention with Linear Biases）（LLaMA，BLOOM）
3. 其他位置编码：
	1. RoPE

RoPE：
1. 核心思想：将位置编码视为对查询（Query）和键（Key）向量的​旋转操作​​。对于向量中的每一对维度（如第2i和2i+1维），构造一个二维旋转矩阵，通过旋转角度θm​将位置信息嵌入到向量中。
2. 两个旋转后的向量点积仅依赖于它们的相对位置差​。$Q^{'}{K^{'}}^T=QR_m(KR_n)^T=QR_m{R_n}^TK^T=QR_mR_{-n}K^T=QR_{m-n}K^T$
	1. 注意力分数仅与相对位置有关，天然支持相对位置编码


```python
def precompute_freqs_cis(dim, end = int(32 * 1024), theta = 1e6): 
	# 频率计算
	freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) 
	# 序列的位置索引，[0, 1, 2, ... end-1]
	t = torch.arange(end, device=freqs.device) 
	freqs = torch.outer(t, freqs).float() 
	freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1) 
	freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1) 
	return freqs_cos, freqs_sin
```

第一行代码，计算的结果即为：$\frac{1}{\theta^{2i/d}}$，这里只取一半，因为向量旋转时<font color="#ff0000">相邻两维度</font>执行一次旋转
- 误区：上述标红的属于理解有误，实际上任意两个维度执行一次旋转也行，不一定要是$x_i,x_{i+1}$，也可以是$x_i,x_{i+seq\_len/2}(i<seq\_len / 2)$，代码给出的即是后一种情况
第三行代码，`torch.outer`外积，接收n，m维向量，生成(end, dim // 2)维矩阵，同时计算出每个位置在每个维度的旋转角度
第四五行代码这样计算是为了高效的进行RoPE运算，也即：$q_t = q \odot cos(t\theta) + q^* \odot sin(t\theta)$，$q^*$是q的逆序复数形式。


```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed
```

## Attention


```python
class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0
        self.n_local_heads = args.num_attention_heads
        self.n_local_kv_heads = self.num_key_value_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
```
