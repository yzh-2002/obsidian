> [参考项目](https://github.com/lipku/metahuman-stream)

数字人模型：
1. ernerf：数字人3D场景重建，自由视角渲染（表现力强一点）
2. musetalk：同步嘴唇，表情和动作（对比wav2lip效果不明显）
3. wav2lip：给定文本，同步视频人物嘴唇

项目启动：
1. 运行srs

## 流媒体
> 一种可以边传边播的媒体传输方式

业务场景：
1. 点播：
2. 直播：rtmp协议
	1. 主播端：
		1. 视频数据采集：AVFoundation
		2. 视频数据处理（水印，美颜...）：GPUImage...
		3. 音视频编码压缩（便于传输）：音频FFmpeg/视频X264
		4. 分发推流：librtmp
	2. 直播服务器端：srs，bms，nginx
		1. 核心功能：收集主播端的视频推流，放大后推送给观众端
		2. 其他功能：鉴权认证，视频连线...
	3. 观众端：
		1. 拉流：rtmp,http-flv,hls...
		2. 音视频解码
		3. 播放
3. 实时通信：webrtc
	1. 支持点对点通信
	2. 一旦建立连接，数据可直接在浏览器和设备之间实时传输，而无需接触服务器
		1. 建立连接之前需要服务器来寻找对方
	3. ...

有关直播和rtc场景各环节技术栈区别，可见[此视频](https://www.bilibili.com/video/BV1M54y1z7jo/?vd_source=43ba5c8376723f2815ebb6eafbedae3e)